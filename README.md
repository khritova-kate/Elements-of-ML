# Elements-of-ML

Проект выполнен в рамках курса естественно-научного содержания (ЕНС) _**"Элементы машинного обучения"**_  
Преподаватель: Шокуров Антон Вячеславович

Проект основан на статьях:  
[1] Samuel L. Smith, David H. P. Turban, Steven Hamblin & Nils Y. Hammerla, [Offline bilingual word vectors, orthogonal transformations and
the inverted softmax](https://openreview.net/pdf?id=r1Aab85gg)   
[2] A. Conneau, G. Lample, L. Denoyer, MA. Ranzato, H. Jégou, [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf)  
[3] Armand Joulin Piotr Bojanowski Tomas Mikolov, [Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion](https://arxiv.org/pdf/1804.07745.pdf)

Рассмотрена задача машинного перевода для заданного представления слов. Для решения задачи предлагается построить некоторое отображение, выравнивающее заданные представления в едином векторном пространстве.

При решении задачи рассмотрены следующие модели:
1. Использующая линейное отображение между непрерывными представлениями слов.
2. Использующая линейное ортогональное отображение между непрерывными представлениями слов.
3. Использующая линейное ортогональное отображение между непрерывными представлениями слов, при построении которого используется корректирующая метрика.

Для исследования выбраны два родственных языка: **русский** и **украинский**. В качестве исходного датасета используется файл, содержащий украинские и русские слова, разделенные табуляцией.

В качестве меры качества будем ипользовать долю "близко угаданных" слов с точностью top-1, top-5 и top-10:  
Для каждого преобразованного украинского представления ищется его N (N = 1, 5, 10) ближайших соседей в пространстве представления русских слов (т.е. слово переводится с украинского на русский и для полученного перевода ищется N наиболее близких по смыслу русских слов). Далее находится частота "близко угаданных" слов, т.е. тех, у которых хотя бы одна точка в найденной окрестности совпадает с истинным переводом.

Оценки, полученные для каждой модели, приведены в следующей таблице:

|      | Модель №1 | Модель №2 | Модель №3 |
|------|-----------|-----------|-----------|
|top-1 | 0.6425    | 0.6554    | 0.6606    |
|top-5 | 0.8083    | 0.8238    | 0.8342    |
|top-10| 0.8497    | 0.8523    | 0.8549    |


Модель №1 является худшей, а модель №3 показывает лучший результат.

Также рассмотрена задача классификации текстовых сообщений. В качестве данных используется описание 15 различных слов 8 различными участниками (на украинском языке). Решается задача классификации для сообщений на украинском языке. После чего все сообщения переводятся на русский язык с помощью моделей 1-3 и выполняется такая же классификация.

Если выполнить обучение и прогноз (со случайным разбиением выборки на обучающую и тестовую) 50 раз и взять среднее значение полученных результа, получаются следующие оценки:

|      | Модель №1 | Модель №2 | Модель №3 |
|------|-----------|-----------|-----------|
|Train | 1.0       | 1.0       | 1.0       |
|Test  | 0.9055    | 0.9166    | 0.94      |

Т.е. модель №3 снова показывает лучший результат
